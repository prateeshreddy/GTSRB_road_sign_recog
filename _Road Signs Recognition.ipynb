{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import time\n",
    "import itertools\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plotly.offline.init_notebook_mode(True)\n",
    "\n",
    "dataset_dir = '../input/'\n",
    "meta_info = os.path.join(dataset_dir, 'Meta.csv')\n",
    "train_csv_path = os.path.join(dataset_dir, 'Train.csv')\n",
    "test_csv_path = os.path.join(dataset_dir, 'Test.csv')\n",
    "labels = ['20 km/h', '30 km/h', '50 km/h', '60 km/h', '70 km/h', '80 km/h', '80 km/h end', '100 km/h', '120 km/h', 'No overtaking',\n",
    "               'No overtaking for tracks', 'Crossroad with secondary way', 'Main road', 'Give way', 'Stop', 'Road up', 'Road up for track', 'Brock',\n",
    "               'Other dangerous', 'Turn left', 'Turn right', 'Winding road', 'Hollow road', 'Slippery road', 'Narrowing road', 'Roadwork', 'Traffic light',\n",
    "               'Pedestrian', 'Children', 'Bike', 'Snow', 'Deer', 'End of the limits', 'Only right', 'Only left', 'Only straight', 'Only straight and right', \n",
    "               'Only straight and left', 'Take right', 'Take left', 'Circle crossroad', 'End of overtaking limit', 'End of overtaking limit for track']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_color = '#0f7b8e'\n",
    "test_data_color = '#630f8e'\n",
    "\n",
    "trainDf = pd.read_csv(train_csv_path)\n",
    "testDf = pd.read_csv(test_csv_path)\n",
    "metaDf = pd.read_csv(meta_info)\n",
    "\n",
    "trainDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), trainDf['Path']))\n",
    "testDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), testDf['Path']))\n",
    "metaDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), metaDf['Path']))\n",
    "\n",
    "trainDf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover dataset balance\n",
    "The easiest way to discover dataset balance - build histogram. We consider to use [seaborn](https://seaborn.pydata.org/) library based on matplotlib for pretty data visualization.\n",
    "\n",
    "Train and test subset of dataset have similar balance distribution. Train and test split provided by GTSRB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(25, 6))\n",
    "axs[0].set_title('Train classes distribution')\n",
    "axs[0].set_xlabel('Class')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[1].set_title('Test classes distribution')\n",
    "axs[1].set_xlabel('Class')\n",
    "axs[1].set_ylabel('Count')\n",
    "\n",
    "sns.countplot(trainDf.ClassId, ax=axs[0])\n",
    "sns.countplot(testDf.ClassId, ax=axs[1])\n",
    "axs[0].set_xlabel('Class ID');\n",
    "axs[1].set_xlabel('Class ID');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples visualization\n",
    "\n",
    "It is good idea to visualize samples in order to brief data exploration. Image visualization can help to understand data problem. Some solutions (such as histogram equalization) can be discovered by visual data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 6\n",
    "cols = 12\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 12))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=0.9, wspace=None, hspace=None)\n",
    "visualize = trainDf.sample(rows*cols)\n",
    "\n",
    "idx = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img = cv2.imread(visualize[\"Path\"].tolist()[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (60,60))\n",
    "        axs[i,j].imshow(img)\n",
    "        axs[i,j].set_title(labels[int(visualize[\"ClassId\"].tolist()[idx])])\n",
    "        axs[i,j].get_xaxis().set_visible(False)\n",
    "        axs[i,j].get_yaxis().set_visible(False)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow utils\n",
    "\n",
    "Here we defined utils for loading tensors, augmentation and other data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_load_size = (60,60)\n",
    "zero_img = np.zeros([12,img_load_size[0], img_load_size[1], 3])\n",
    "zero_label = np.zeros([12,1])\n",
    "\n",
    "\n",
    "''' The parse_function will do the following:\n",
    "\n",
    "read the content of the file\n",
    "decode using jpeg format\n",
    "convert to float values in [0, 1]\n",
    "resize to size (64, 64) ''' \n",
    "\n",
    "def parse_function(filename, label):\n",
    "    \n",
    "        image_string = tf.read_file(filename)\n",
    "        image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        image.set_shape([None, None, 3])\n",
    "        \n",
    "        return filename, image, label    \n",
    "    \n",
    "def train_preprocess(filename, image, label):\n",
    "    '''  train_preprocess can be used during training to perform data augmentation:\n",
    "\n",
    "                 *Horizontally flip the image with probability 1/2\n",
    "                 *Apply random brightness and saturation\n",
    "    '''\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize_images(image, img_load_size)\n",
    "    return filename, image, label\n",
    "\n",
    "def augmentate(filename, image, label):\n",
    "    \n",
    "    '''\n",
    "    The amount of data we have is not sufficient for a model to generalise well.\n",
    "    It is also fairly unbalanced, and some classes are represented to significantly lower extent than the others.\n",
    "    But we will fix this with data augmentation\n",
    "    * Flipping\n",
    "    * Rotation and projection\n",
    "    '''\n",
    "    \n",
    "    grad = tf.random.uniform(shape=[], minval=-0.3, maxval=0.3)\n",
    "    dx = tf.random.uniform(shape=[], minval=-15, maxval=15, dtype=tf.int32)\n",
    "    dy = tf.random.uniform(shape=[], minval=-15, maxval=15, dtype=tf.int32)\n",
    "    image = tf.contrib.image.rotate(image, grad)\n",
    "    image = tf.contrib.image.translate(image, translations=[dx, dy])\n",
    "    \n",
    "    return filename, image, label\n",
    "\n",
    "def eq(img: np.ndarray):\n",
    "    res = img.copy()\n",
    "    res[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "    res[:, :, 1] = cv2.equalizeHist(img[:, :, 1])\n",
    "    res[:, :, 2] = cv2.equalizeHist(img[:, :, 2])\n",
    "    \n",
    "    return res\n",
    "\n",
    "def tf_equalize_histogram(image):\n",
    "    \n",
    "    ''' Histogram Equalization is a computer image processing technique used to improve contrast in images.\n",
    "    It accomplishes this by effectively spreading out the most frequent intensity values,\n",
    "                                     i.e. stretching out the intensity range of the image\n",
    "    '''\n",
    "    \n",
    "    values_range = tf.constant([0., 255.], dtype = tf.float32)\n",
    "    histogram = tf.histogram_fixed_width(tf.to_float(image), values_range, 256)\n",
    "    cdf = tf.cumsum(histogram)\n",
    "    cdf_min = cdf[tf.reduce_min(tf.where(tf.greater(cdf, 0)))]\n",
    "\n",
    "    img_shape = tf.shape(image)\n",
    "    pix_cnt = img_shape[-3] * img_shape[-2]\n",
    "    px_map = tf.round(tf.to_float(cdf - cdf_min) * 255. / tf.to_float(pix_cnt - 1))\n",
    "    px_map = tf.cast(px_map, tf.uint8)\n",
    "\n",
    "    gth = tf.gather_nd(px_map, tf.cast(image, tf.int32))\n",
    "    eq_hist = tf.expand_dims(gth, 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow data pipeline\n",
    "\n",
    "We decide to use native tensorflow as deep learning framework without high level API. The best way to implement data pipleline - implement load data operation as node of computation graph. This will be the first node of computitional graph (i.e. all other node is depended from it). This approach has several advantages:\n",
    "\n",
    "* Parallelism out of the box (provided by dataflow graph principies)\n",
    "* Opportunity to implement data augmentation on GPU \n",
    "* Convenient way to wrok with it provided by tf.data infrastructure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 12\n",
    "prefetch_count = 1\n",
    "samples_train = len(trainDf)\n",
    "samples_test = len(testDf)\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((trainDf['Path'], trainDf['ClassId']))\n",
    "dataset_train = dataset_train.shuffle(len(trainDf['Path']))\n",
    "dataset_train = dataset_train.repeat(epochs)\n",
    "dataset_train = dataset_train.map(parse_function, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.map(train_preprocess, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.map(augmentate, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(prefetch_count)\n",
    "\n",
    "dataset_iterator = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                                          dataset_train.output_shapes)\n",
    "\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((testDf['Path'], testDf['ClassId']))\n",
    "dataset_test = dataset_test.shuffle(len(testDf['Path']))\n",
    "dataset_test = dataset_test.repeat(epochs+1)\n",
    "dataset_test = dataset_test.map(parse_function, num_parallel_calls=4)\n",
    "dataset_test = dataset_test.map(train_preprocess, num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "dataset_test = dataset_test.prefetch(prefetch_count)\n",
    "\n",
    "\n",
    "train_init_op = dataset_iterator.make_initializer(dataset_train)\n",
    "test_init_op = dataset_iterator.make_initializer(dataset_test)\n",
    "\n",
    "load_filename, load_img, load_label = dataset_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=8, nrows=1, figsize=(15, 6))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op)\n",
    "    for j in range(8):\n",
    "        i, l = sess.run([load_img, load_label])\n",
    "        i = (i[0]*255).astype(np.uint8)\n",
    "        ax[j].imshow(i)\n",
    "        ax[j].set_title(labels[l[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_rate = tf.placeholder(dtype=tf.float32, shape=[], name='dp_rate')\n",
    "\n",
    "img_placeholder = tf.placeholder(shape=[None, 60,60,3], dtype=tf.float32, name='img_placeholder')\n",
    "label_placeholder = tf.placeholder(shape=[None, 1], dtype=tf.int64, name='label_placeholder')\n",
    "manual_load = tf.placeholder(dtype=tf.bool, shape=[], name='manual_load_placeholder')\n",
    "\n",
    "# inp = net = tf.cond(pred=manual_load, true_fn=lambda : img_placeholder, false_fn=lambda : load_img, name='network_start')\n",
    "# label = tf.cond(pred=manual_load, true_fn=lambda : label_placeholder, false_fn=lambda : load_label, name='label')\n",
    "\n",
    "inp = net = tf.cond(manual_load, lambda: img_placeholder, lambda: load_img)\n",
    "label = load_label\n",
    "\n",
    "conv1 = net = tf.layers.conv2d(inputs=net, filters=16, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "conv2 = net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "conv3 = net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "conv4 = net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "net = tf.layers.max_pooling2d(inputs=net, pool_size=(2,2), strides=(2,2))\n",
    "\n",
    "conv5 = net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "conv6 = net = tf.layers.conv2d(inputs=net, filters=128, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "conv5 = net = tf.layers.conv2d(inputs=net, filters=256, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "conv6 = net = tf.layers.conv2d(inputs=net, filters=400, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "flatten1 = net = tf.layers.flatten(inputs=net)\n",
    "\n",
    "dp1 = net = tf.layers.dropout(inputs=net, rate=dp_rate)\n",
    "dense1 = net = tf.layers.dense(inputs=net, units=256)\n",
    "logits = tf.layers.dense(inputs=net, units=43)\n",
    "\n",
    "pred_classes = tf.argmax(logits, axis=1)\n",
    "pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "acc, acc_op = tf.metrics.accuracy(labels=label, predictions=pred_classes)\n",
    "end_loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=label)\n",
    "\n",
    "loss = end_loss\n",
    "\n",
    "label_transpose = tf.transpose(label)\n",
    "correct_prediction = tf.equal(pred_classes, label_transpose)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "confusion_matrix_op = tf.confusion_matrix(labels=label, predictions=pred_classes, num_classes=43)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Train model several epoch and store results of each epoch for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_history = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}\n",
    "best_acc = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    epoch_history = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}\n",
    "    \n",
    "    sess.run(train_init_op)\n",
    "    for i in tqdm.tqdm_notebook(range(samples_train//batch_size), ascii=True, desc='Train epoch {}'.format(e)):\n",
    "        _, _loss, _acc, mn = sess.run([opt, loss, accuracy, inp], feed_dict={dp_rate: 0.3, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n",
    "#         print(np.mean(mn))\n",
    "        epoch_history['loss'].append(_loss)\n",
    "        epoch_history['acc'].append(_acc)\n",
    "        \n",
    "    sess.run(test_init_op)\n",
    "    for i in tqdm.tqdm_notebook(range(samples_test//batch_size), ascii=True, desc='Test epoch {}'.format(e)):\n",
    "        _loss, _acc = sess.run([loss, accuracy], feed_dict={dp_rate: 0, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n",
    "        epoch_history['val_loss'].append(_loss)\n",
    "        epoch_history['val_acc'].append(_acc)\n",
    "        \n",
    "    train_history['loss'].append(np.mean(epoch_history['loss']))\n",
    "    train_history['acc'].append(np.mean(epoch_history['acc']))\n",
    "    train_history['val_loss'].append(np.mean(epoch_history['val_loss']))\n",
    "    train_history['val_acc'].append(np.mean(epoch_history['val_acc']))\n",
    "    \n",
    "    print(\"***EPOCH SUMMARY*** Loss: {} Acc: {} | Test Loss: {} Test Acc {}\".format(train_history['loss'][-1], train_history['acc'][-1],\n",
    "                                                                                    train_history['val_loss'][-1], train_history['val_acc'][-1]))\n",
    "\n",
    "    if train_history['val_acc'][-1] > best_acc:\n",
    "        best_acc = train_history['val_acc'][-1]\n",
    "        save_path = saver.save(sess, \"./model.ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlefont = dict(family='Courier New, monospace', size=18, color='#7f7f7f')\n",
    "layout = go.Layout(title='Traing & Test loss', xaxis=dict(title='Epoch', titlefont=titlefont),\n",
    "                                    yaxis=dict(title='Loss', titlefont=titlefont))\n",
    "fig = go.Figure(data=[go.Scatter(y=train_history['loss'], name='Train loss'), go.Scatter(y=train_history['val_loss'], name='Test loss')], layout=layout)\n",
    "plotly.offline.iplot(fig)\n",
    "\n",
    "layout = go.Layout(title='Traing & Test accuracy', xaxis=dict(title='Epoch', titlefont=titlefont),\n",
    "                                    yaxis=dict(title='Accuracy', titlefont=titlefont))\n",
    "fig = go.Figure(data=[go.Scatter(y=train_history['acc'], name='Train accuracy'), go.Scatter(y=train_history['val_acc'], name='Test accuracy')], layout=layout)\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance data preparation\n",
    "\n",
    "For futher model analysis we need some data. Very good solution - store all statistic data in pandas DataFrame data structure. Let's evaluate all test samples and store all information about prediction, probabilities and other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, \"./model.ckpt\")\n",
    "sess.run(test_init_op)\n",
    "confusion_matrix = np.zeros([43,43])\n",
    "test_analys = trainDf.copy()\n",
    "predictions = []\n",
    "probabilities = []\n",
    "analys = []\n",
    "\n",
    "for i in tqdm.tqdm_notebook(range(samples_test//batch_size), ascii=True, desc='Test best model'):\n",
    "    _files, _predictions, _probas, _gts, _cm = sess.run([load_filename, pred_classes, pred_probas, load_label, confusion_matrix_op], feed_dict={dp_rate: 0, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n",
    "    confusion_matrix += _cm\n",
    "    for i in range(batch_size):\n",
    "        sample_info = {'image': _files[i].decode(), 'prediction': int(_predictions[i]), 'gt': int(_gts[i]), 'gt_probas': _probas[i][_gts[i]],\n",
    "                       'prediction_probas': _probas[i][_predictions[i]], 'prediction_type': 'Correct' if _gts[i] == _predictions[i] else 'Wrong'}\n",
    "        for cls_id, j in enumerate(_probas[i]):\n",
    "            sample_info['prob_{}'.format(cls_id)] = j\n",
    "        analys.append(sample_info)\n",
    "\n",
    "analys_df = pd.DataFrame(analys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation analys overview\n",
    "\n",
    "We have built pandas DataFrame. Let's observe it structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analys_df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random prediction visualization\n",
    "\n",
    "Using information computed above, we can visualize some random samples with their predictions. As we can see - it is impossible to recognize some pictures definitely by human, but network still generates correct predictions. Awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 4\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 8))\n",
    "visualize = trainDf.sample(rows*cols)\n",
    "\n",
    "analys_df_copy = analys_df.copy()\n",
    "analys_df_copy = analys_df_copy.sample(frac=1)\n",
    "\n",
    "idx = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img = cv2.imread(analys_df_copy.iloc[idx]['image'])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (100, 100))\n",
    "        \n",
    "        gt = analys_df_copy.iloc[idx]['gt']\n",
    "        pred = analys_df_copy.iloc[idx]['prediction']\n",
    "        \n",
    "        axs[i,j].imshow(img)\n",
    "        axs[i,j].set_title('Predicted: {}\\nGround truth {}'.format(labels[pred], labels[gt]), fontsize=14)\n",
    "        axs[i,j].get_xaxis().set_visible(False)\n",
    "        axs[i,j].get_yaxis().set_visible(False)\n",
    "        idx += 1\n",
    "        \n",
    "fig.suptitle(\"Random prediction\", fontsize=30, y=2.1, x=0.515);\n",
    "plt.subplots_adjust(left=None, bottom=None, right=0.9, top=1.9, wspace=None, hspace=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong prediction visualization\n",
    "\n",
    "Our model can't achieve perfect accuracy (i.e. 100%). Let's visualize wrong predicted samples. Some of them have realy bad quality, resolution. Others have unexpected artifacts (such as extreame rotation, half hidden signs or shaddow). This situations wasn't present in train part, so network have no idea how to deal with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 4\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 8))\n",
    "visualize = trainDf.sample(rows*cols)\n",
    "\n",
    "analys_df_copy = analys_df[analys_df['prediction_type'] == 'Wrong'].copy()\n",
    "analys_df_copy = analys_df_copy.sample(frac=1)\n",
    "\n",
    "idx = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img = cv2.imread(analys_df_copy.iloc[idx]['image'])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (100, 100))\n",
    "        \n",
    "        gt = analys_df_copy.iloc[idx]['gt']\n",
    "        pred = analys_df_copy.iloc[idx]['prediction']\n",
    "        \n",
    "        axs[i,j].imshow(img)\n",
    "        axs[i,j].set_title('Predicted: {}\\nGround truth {}'.format(labels[pred], labels[gt]), fontsize=14)\n",
    "        axs[i,j].get_xaxis().set_visible(False)\n",
    "        axs[i,j].get_yaxis().set_visible(False)\n",
    "        idx += 1\n",
    "        \n",
    "fig.suptitle(\"Wrong prediction\", fontsize=30, y=2.1, x=0.515);\n",
    "plt.subplots_adjust(left=None, bottom=None, right=0.9, top=1.9, wspace=None, hspace=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Confusion matrix gives us additional information about accuracy distribution. It's naturally that network may be confused in prediction 'Pedestrian' and 'Other dangerous' signs. Also, confusion matrix can give us idea what to improove in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix\n",
    "f = np.sum(cm, axis=1)\n",
    "normalized_cm = cm\n",
    "for i in range(43):\n",
    "    normalized_cm[i, :] /= sum(normalized_cm[i, :])\n",
    "\n",
    "normalized_cm = np.round(normalized_cm, 2)\n",
    "    \n",
    "fig, ax = plt.subplots(1,1, figsize=((20, 20)))\n",
    "\n",
    "ax.imshow(normalized_cm)\n",
    "\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(labels)):\n",
    "   for j in range(len(labels)):\n",
    "       ax.text(j, i, normalized_cm[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title('Confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analys_df.prediction_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are histogram of prediction types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, sharex=False, sharey=True, figsize=(25, 7))\n",
    "\n",
    "px = sns.countplot(x='prediction_type', data=analys_df, ax=axs)\n",
    "axs.set_title('Prediction type distribution', fontsize=18)\n",
    "axs.set_xlabel('Prediction type', fontsize=16)\n",
    "axs.set_ylabel('Fraction', fontsize=16);\n",
    "\n",
    "\n",
    "total = analys_df.shape[0]\n",
    "for idx, p in enumerate(px.patches):\n",
    "        px.annotate('{:.1f}%'.format(p.get_height()/total*100), (p.get_x()+0.365, p.get_height()+100), fontsize=18)\n",
    "\n",
    "\n",
    "px.yaxis.set_ticks(np.linspace(0, total, 11))\n",
    "px.set_yticklabels(map('{:.1f}%'.format, 100*px.yaxis.get_majorticklocs()/total));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
